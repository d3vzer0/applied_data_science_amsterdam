{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/logo_white_bkg_small.png\" align=\"right\" />\n",
    "\n",
    "#  Automate it All! - Answers\n",
    "This worksheet covers concepts relating to automating a machine learning model using the techniques we learned.  It should take no more than 20-30 minutes to complete.  Please raise your hand if you get stuck.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T15:21:16.151789Z",
     "start_time": "2019-02-27T15:21:16.145483Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scikitplot as skplt\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import lime\n",
    "from tpot import TPOTClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step One:  Import the Data\n",
    "In this example, we're going to use the dataset we used in worksheet 5.3.  Run the following code to read in the data, extract the features and target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T04:55:31.723676Z",
     "start_time": "2019-02-27T04:55:31.708177Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/dga_features_final_df.csv')\n",
    "target = df_final['isDGA']\n",
    "feature_matrix = df_final.drop(['isDGA'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, perform the test/train split in the conventional manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T04:55:32.377673Z",
     "start_time": "2019-02-27T04:55:32.370508Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_matrix_train, feature_matrix_test, target_train, target_test = train_test_split(feature_matrix, \n",
    "                                                                                        target, \n",
    "                                                                                        test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Two:  Run the Optimizer\n",
    "In the next step, use TPOT to create a classification pipeline using the DGA data set that we have been using.  The `TPOTClassifier()` has many configuration options and in the interest of time, please set the following variables when you instantiate the classifier.\n",
    "\n",
    "* `max_eval_time_mins`:  In the interests of time, set this to 15 or 20.\n",
    "* `verbosity`: Set to 1 or 2 so you can see what TPOT is doing.\n",
    "\n",
    "\n",
    "**Note:  This step will take some time, so you might want to get some coffee or a snack when it is running.**  While this is running take a look at the other configuration options available here: http://epistasislab.github.io/tpot/api/.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T06:03:30.680093Z",
     "start_time": "2019-02-27T04:59:00.937173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: xgboost.XGBClassifier is not available and will not be used by TPOT.\n",
      "29 operators have been imported by TPOT.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Optimization Progress', max=10100, style=ProgressStyle(descriâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _mate_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _mate_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _mate_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _mate_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _mate_operator: num_test=4 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _mate_operator: num_test=5 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 82\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 1 - Current Pareto front scores:\n",
      "-1\t0.9073373481927577\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=12, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9093306888595057\tRandomForestClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=6, RandomForestClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 85\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 2 - Current Pareto front scores:\n",
      "-1\t0.9080040148594243\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=14, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9093306888595057\tRandomForestClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=6, RandomForestClassifier__n_estimators=100)\n",
      "-4\t0.9100040296744039\tGaussianNB(RandomForestClassifier(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.3, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8500000000000001), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=5, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.1), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.1, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 77\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 3 - Current Pareto front scores:\n",
      "-1\t0.9080040148594243\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=14, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9093306888595057\tRandomForestClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), RandomForestClassifier__bootstrap=True, RandomForestClassifier__criterion=entropy, RandomForestClassifier__max_features=0.5, RandomForestClassifier__min_samples_leaf=6, RandomForestClassifier__min_samples_split=6, RandomForestClassifier__n_estimators=100)\n",
      "-4\t0.9100040296744039\tGaussianNB(RandomForestClassifier(GradientBoostingClassifier(RFE(input_matrix, RFE__ExtraTreesClassifier__criterion=entropy, RFE__ExtraTreesClassifier__max_features=0.3, RFE__ExtraTreesClassifier__n_estimators=100, RFE__step=0.8500000000000001), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.9000000000000001, GradientBoostingClassifier__min_samples_leaf=15, GradientBoostingClassifier__min_samples_split=5, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.1), RandomForestClassifier__bootstrap=False, RandomForestClassifier__criterion=gini, RandomForestClassifier__max_features=0.1, RandomForestClassifier__min_samples_leaf=17, RandomForestClassifier__min_samples_split=14, RandomForestClassifier__n_estimators=100))\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53\n",
      "Generation 4 - Current Pareto front scores:\n",
      "-1\t0.9080040148594243\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=True, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=14, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9106773556743223\tExtraTreesClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=7, ExtraTreesClassifier__min_samples_split=12, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 5 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 6 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 7 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 8 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 9 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 10 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 11 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 12 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 74\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Generation 13 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 array must not contain infs or NaNs\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66\n",
      "Generation 14 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Generation 15 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52\n",
      "Generation 16 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 17 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 89\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 18 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 19 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 20 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 21 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 22 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 67\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 23 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 53\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 1 feature(s) (shape=(50, 1)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 24 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 25 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 26 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 27 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9126729408104535\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=0.001, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=entropy, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 28 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 29 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9120018296499589\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=9, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 30 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 31 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "Generation 32 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 33 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 34 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 2 is required by FeatureAgglomeration.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.00500\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Generation 35 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 36 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 37 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 38 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 95\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "Generation 39 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 40 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 88\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 41 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 42 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 43 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 44 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 76\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 45 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 46 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 47 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 48 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 66\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 49 - Current Pareto front scores:\n",
      "-1\t0.9120018000200002\tExtraTreesClassifier(input_matrix, ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=9, ExtraTreesClassifier__n_estimators=100)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 60\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 50 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 55\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 51 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MaxAbsScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 52\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "Generation 52 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 53 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 54 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 55 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 56 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 64\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 57 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 72\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 58 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 59 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9126684963166257\tGradientBoostingClassifier(PCA(input_matrix, PCA__iterated_power=2, PCA__svd_solver=randomized), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=5, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=7, GradientBoostingClassifier__min_samples_split=20, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 60 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 62\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 97\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 61 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=3 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 62 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 63 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 64 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 70\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 65 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 66 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 67 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 68 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 69 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9133395926621407\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=9, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 83\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 70 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 71 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 72 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 73 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 74 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required by MinMaxScaler.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 75 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 93\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 76 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 77 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Generation 78 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 79 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 61\n",
      "Generation 80 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 98\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 81 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.05000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 81\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 82 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 83 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 84 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 85 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 51\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 86 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 86\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 100\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Generation 87 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 94\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 88 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l1 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 78\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 89 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 69\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 73\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "Generation 90 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 75\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 91 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 91\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 57\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 92 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-5\t0.915330733304444\tExtraTreesClassifier(SelectFwe(LinearSVC(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), LinearSVC__C=20.0, LinearSVC__dual=True, LinearSVC__loss=hinge, LinearSVC__penalty=l2, LinearSVC__tol=0.0001), SelectFwe__alpha=0.033), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 93 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-5\t0.915330733304444\tExtraTreesClassifier(SelectFwe(LinearSVC(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), LinearSVC__C=20.0, LinearSVC__dual=True, LinearSVC__loss=hinge, LinearSVC__penalty=l2, LinearSVC__tol=0.0001), SelectFwe__alpha=0.033), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 87\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=2 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 94 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-4\t0.915330733304444\tExtraTreesClassifier(LinearSVC(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), LinearSVC__C=20.0, LinearSVC__dual=True, LinearSVC__loss=hinge, LinearSVC__penalty=l2, LinearSVC__tol=0.0001), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 92\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 58\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 95 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-4\t0.915330733304444\tExtraTreesClassifier(LinearSVC(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), LinearSVC__C=20.0, LinearSVC__dual=True, LinearSVC__loss=hinge, LinearSVC__penalty=l2, LinearSVC__tol=0.0001), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.5, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 manhattan was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l2' and loss='hinge' are not supported when dual=False, Parameters: penalty='l2', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 96 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-4\t0.9153373778227165\tGradientBoostingClassifier(OneHotEncoder(StandardScaler(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2)), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 97 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-4\t0.9153373778227165\tGradientBoostingClassifier(OneHotEncoder(StandardScaler(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2)), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='squared_hinge' are not supported when dual=True, Parameters: penalty='l1', loss='squared_hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 79\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 l2 was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 98 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-4\t0.9153373778227165\tGradientBoostingClassifier(OneHotEncoder(StandardScaler(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2)), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 68\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 cosine was provided as affinity. Ward can only work with euclidean distances.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 X contains negative values.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 99 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-4\t0.9153373778227165\tGradientBoostingClassifier(OneHotEncoder(StandardScaler(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2)), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_pre_test decorator: _random_mutation_operator: num_test=0 No feature in X meets the variance threshold 0.20000\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=1 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=False\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Found array with 0 feature(s) (shape=(50, 0)) while a minimum of 1 is required.\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 59\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='hinge' is not supported, Parameters: penalty='l1', loss='hinge', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 56\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Expected n_neighbors <= n_samples,  but n_samples = 50, n_neighbors = 80\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "_pre_test decorator: _random_mutation_operator: num_test=0 Input X must be non-negative\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Pipeline encountered that has previously been evaluated during the optimization process. Using the score from the previous evaluation.\n",
      "Generation 100 - Current Pareto front scores:\n",
      "-1\t0.9120062593288074\tGradientBoostingClassifier(input_matrix, GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=6, GradientBoostingClassifier__max_features=0.1, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=19, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-2\t0.9140040296744039\tGradientBoostingClassifier(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "-3\t0.9146707259710294\tExtraTreesClassifier(BernoulliNB(VarianceThreshold(input_matrix, VarianceThreshold__threshold=0.2), BernoulliNB__alpha=10.0, BernoulliNB__fit_prior=True), ExtraTreesClassifier__bootstrap=False, ExtraTreesClassifier__criterion=gini, ExtraTreesClassifier__max_features=0.55, ExtraTreesClassifier__min_samples_leaf=2, ExtraTreesClassifier__min_samples_split=5, ExtraTreesClassifier__n_estimators=100)\n",
      "-4\t0.9153373778227165\tGradientBoostingClassifier(OneHotEncoder(StandardScaler(LogisticRegression(input_matrix, LogisticRegression__C=0.01, LogisticRegression__dual=True, LogisticRegression__penalty=l2)), OneHotEncoder__minimum_fraction=0.15, OneHotEncoder__sparse=False, OneHotEncoder__threshold=10), GradientBoostingClassifier__learning_rate=0.01, GradientBoostingClassifier__max_depth=7, GradientBoostingClassifier__max_features=0.2, GradientBoostingClassifier__min_samples_leaf=2, GradientBoostingClassifier__min_samples_split=3, GradientBoostingClassifier__n_estimators=100, GradientBoostingClassifier__subsample=0.7500000000000001)\n",
      "\n",
      "\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TPOTClassifier(config_dict=None, crossover_rate=0.1, cv=5,\n",
       "        disable_update_check=False, early_stop=None, generations=100,\n",
       "        max_eval_time_mins=5, max_time_mins=None, memory=None,\n",
       "        mutation_rate=0.9, n_jobs=-1, offspring_size=None,\n",
       "        periodic_checkpoint_folder=None, population_size=100,\n",
       "        random_state=None, scoring=None, subsample=1.0, use_dask=False,\n",
       "        verbosity=3, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here... \n",
    "optimizer = TPOTClassifier(n_jobs=-1, verbosity=3)\n",
    "optimizer.fit(feature_matrix_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Three:  Evaluate the Performance\n",
    "Now that you have a trained model, the next step is to evaluate the performance and see how TPOT did in comparison with earlier models we created.  Use the techniques you've learned to evaluate the performance of your model.  Specifically, print out the `classification report` and a confusion matrix. \n",
    "\n",
    "Unfortunately, Yellowbrick will not work in this instance, however, you can generate a similar visual confusion matrix with the following code:\n",
    "\n",
    "```\n",
    "import scikitplot as skplt\n",
    "skplt.metrics.confusion_matrix(optimized_preds, target_test)\n",
    "\n",
    "```\n",
    "\n",
    "What is the accuracy of your model?  Is it significantly better than what you did in earlier labs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T15:21:37.710556Z",
     "start_time": "2019-02-27T15:21:37.700287Z"
    }
   },
   "outputs": [],
   "source": [
    "predictions = optimizer.predict(feature_matrix_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T15:22:07.825701Z",
     "start_time": "2019-02-27T15:22:07.817113Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.89      0.92       266\n",
      "           1       0.88      0.96      0.92       234\n",
      "\n",
      "   micro avg       0.92      0.92      0.92       500\n",
      "   macro avg       0.92      0.92      0.92       500\n",
      "weighted avg       0.92      0.92      0.92       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T15:21:39.108636Z",
     "start_time": "2019-02-27T15:21:38.948510Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a2cd5e1d0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "skplt.metrics.plot_confusion_matrix(optimized_preds, target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4:  Export your Pipeline\n",
    "If you are happy with the results from `TPOT` you can export the pipeline as python code. The final step in this lab is to export the pipeline as a file called `automate_ml.py` and examine it.  What model and preprocessing steps did TPOT find?  Was this a surprise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-27T15:26:03.601289Z",
     "start_time": "2019-02-27T15:26:03.596273Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.export('automate_ml.py')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
