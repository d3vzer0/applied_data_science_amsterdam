{"paragraphs":[{"text":"%pyspark\n# PySpark\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\nfrom pyspark.sql import Row\n\n# Python\nimport json\nimport pandas as pd\nimport numpy as np\nimport re, sys, os\nfrom datetime import datetime","dateUpdated":"2017-07-13T02:02:32+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1499907490826_-197721996","id":"20170708-174859_170105400","dateCreated":"2017-07-13T01:58:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9133","user":"anonymous","dateFinished":"2017-07-13T02:02:32+0100","dateStarted":"2017-07-13T02:02:32+0100"},{"text":"%md\n\n# Apache Log Parser PySpark (Batch Analysis)\n\nRead Apache Log file from disk as [rdd](http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds) which gives better low-level control to map each line according to regex pattern. Convert to [PySpark DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html) to perform additional ETL using the sqlContext. \n","dateUpdated":"2017-07-13T02:02:32+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Apache Log Parser PySpark (Batch Analysis)</h1>\n<p>Read Apache Log file from disk as <a href=\"http://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\">rdd</a> which gives better low-level control to map each line according to regex pattern. Convert to <a href=\"http://spark.apache.org/docs/latest/sql-programming-guide.html\">PySpark DataFrame</a> to perform additional ETL using the sqlContext.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1499907490827_-198106744","id":"20170708-174916_1629182671","dateCreated":"2017-07-13T01:58:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9134","user":"anonymous","dateFinished":"2017-07-13T02:02:32+0100","dateStarted":"2017-07-13T02:02:32+0100"},{"text":"%pyspark\n\n# Version 1: Parsing of Apache Logs via RDD\nregex_pattern = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)'\n\ndef map_apache_log(line):\n\n    match = re.search(regex_pattern, line)\n    if match is None:\n        log_row = Row(\n            Host='null',\n            ClientID='null',\n            UserID='null',\n            DateTime='null',\n            Method='null',\n            Endpoint='null',\n            Protocol='null',\n            Response='null',\n            SizeBytes='null')\n    else:\n        log_row = Row(\n                Host=match.group(1),\n                ClientID=match.group(2),\n                UserID=match.group(3),\n                DateTime=match.group(4),\n                Method=match.group(5),\n                Endpoint=match.group(6),\n                Protocol=match.group(7),\n                Response=match.group(8),\n                SizeBytes=match.group(9))\n\n    return log_row\n","dateUpdated":"2017-07-13T02:02:32+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1499907490827_-198106744","id":"20170708-175013_2042955683","dateCreated":"2017-07-13T01:58:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9135","user":"anonymous","dateFinished":"2017-07-13T02:02:32+0100","dateStarted":"2017-07-13T02:02:32+0100"},{"text":"%pyspark\n\nDATA_HOME = '/home/griffonuser/applied-ds/data/'\n\nfname = DATA_HOME + 'apache-access.log'\n\n# read rdd, map each row (see function above) and convert to DataFrame\nrdd = sc.textFile(fname).map(map_apache_log)\ndf_from_rdd = rdd.toDF()\ndf_from_rdd = df_from_rdd.na.drop(how='any')\n\n# Define data types\nto_datetime = udf(lambda x: datetime.strptime(x, '%d/%b/%Y:%H:%M:%S %z'), TimestampType())\n\ndf_from_rdd = df_from_rdd.withColumn(\"Response\", df_from_rdd.Response.cast(IntegerType())) \\\n                         .withColumn(\"SizeBytes\", df_from_rdd.SizeBytes.cast(IntegerType()))\ndf_from_rdd = df_from_rdd.na.drop(how='any')\ndf_from_rdd = df_from_rdd.withColumn(\"DateTime\", to_datetime(col(\"DateTime\")))\n\ndf_from_rdd.show()\ndf_from_rdd.printSchema()\n","dateUpdated":"2017-07-13T02:02:32+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------+--------------------+--------------------+---------------+------+--------+--------+---------+------+\n|ClientID|            DateTime|            Endpoint|           Host|Method|Protocol|Response|SizeBytes|UserID|\n+--------+--------------------+--------------------+---------------+------+--------+--------+---------+------+\n|       -|2012-08-03 17:09:...|/html/quickcrossf...| 208.115.113.85|   GET|HTTP/1.1|     200|     8770|     -|\n|       -|2012-08-03 17:09:...|/geotechnical/pro...| 208.115.113.85|   GET|HTTP/1.1|     200|    20813|     -|\n|       -|2012-08-03 17:09:...|/geotechnical/pro...| 208.115.113.85|   GET|HTTP/1.1|     200|    17070|     -|\n|       -|2012-08-03 17:09:...|/geotechnical/pro...| 208.115.113.85|   GET|HTTP/1.1|     200|    16885|     -|\n|       -|2012-08-03 17:09:...|/geotechnical/pro...| 208.115.113.85|   GET|HTTP/1.1|     200|    16077|     -|\n|       -|2012-08-03 17:23:...|/geotechnical/ind...|  65.52.110.146|   GET|HTTP/1.1|     200|     3664|     -|\n|       -|2012-08-03 17:48:...|/geotechnical/pro...|213.186.119.134|   GET|HTTP/1.1|     200|     4217|     -|\n|       -|2012-08-03 17:49:...|         /robots.txt|    66.249.73.3|   GET|HTTP/1.1|     404|      229|     -|\n|       -|2012-08-03 18:00:...|/geotechnical/acc...|    180.76.6.26|   GET|HTTP/1.1|     302|       26|     -|\n|       -|2012-08-03 18:02:...|         /robots.txt|  123.125.71.87|   GET|HTTP/1.1|     404|      227|     -|\n|       -|2012-08-03 18:15:...|         /robots.txt|    1.202.218.8|   GET|HTTP/1.0|     404|      272|     -|\n|       -|2012-08-03 18:25:...|         /robots.txt|  65.52.109.149|   GET|HTTP/1.1|     404|      276|     -|\n|       -|2012-08-03 18:33:...|/geotechnical/ind...|213.186.119.141|   GET|HTTP/1.1|     200|     3203|     -|\n|       -|2012-08-03 18:40:...|         /robots.txt|  207.46.13.203|   GET|HTTP/1.1|     404|      276|     -|\n|       -|2012-08-03 18:40:...|/geotechnical/pri...|  207.46.13.203|   GET|HTTP/1.1|     200|     2749|     -|\n|       -|2012-08-03 18:51:...|/geotechnical/ind...|  65.52.109.149|   GET|HTTP/1.1|     200|     3881|     -|\n|       -|2012-08-03 19:18:...|/geotechnical/pro...| 213.186.127.28|   GET|HTTP/1.1|     200|     3889|     -|\n|       -|2012-08-03 19:30:...|         /robots.txt| 207.46.204.232|   GET|HTTP/1.1|     404|      276|     -|\n|       -|2012-08-03 19:42:...|         /robots.txt|    1.202.218.8|   GET|HTTP/1.0|     404|      276|     -|\n|       -|2012-08-03 19:53:...|/geotechnical/ind...| 207.46.204.232|   GET|HTTP/1.1|     200|     2864|     -|\n+--------+--------------------+--------------------+---------------+------+--------+--------+---------+------+\nonly showing top 20 rows\n\nroot\n |-- ClientID: string (nullable = true)\n |-- DateTime: timestamp (nullable = true)\n |-- Endpoint: string (nullable = true)\n |-- Host: string (nullable = true)\n |-- Method: string (nullable = true)\n |-- Protocol: string (nullable = true)\n |-- Response: integer (nullable = true)\n |-- SizeBytes: integer (nullable = true)\n |-- UserID: string (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1499907490828_-200030489","id":"20170708-181558_1098635770","dateCreated":"2017-07-13T01:58:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9136","user":"anonymous","dateFinished":"2017-07-13T02:02:33+0100","dateStarted":"2017-07-13T02:02:32+0100"},{"text":"%md\n# Apache Log Parser PySpark (Structured Streaming + Kafka)\n\nStart and create Kafka Topic:\n\n```\n# KAFKA\n\n1. $KAFKA_HOME/bin/zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties (Start Zookeeper)\n2. $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties (Start Kafka Server)\nIn GRIFFON skip step 1 and 2 and simply go to Applications > Big Data > Start Kafka\n3. $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testlogs (Create example topic \"testlogs\")\ncd applied-ds\n4. $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testlogs < data/stream/apache-access.log (Initialize Kafka Producer)\n```\n\n\n```\n# Spark\n\n5. Spark subscribe to Kafka topic using the Structured Streaming API (Kafka Consumer)\n6. Spark DataFrame manipulations of incoming stream of messages\n7. Spark Sink out (various options)\n```\n\nDocs:\n\n- Kafka Quick Start (https://kafka.apache.org/quickstart)\n- Spark Structured Streaming in micro-batches (http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html)\n- Kafka Integration (http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html)\n- Zeppelin interpreter settings (https://zeppelin.apache.org/docs/0.5.0-incubating/interpreter/spark.html)","dateUpdated":"2017-07-13T02:02:32+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Apache Log Parser PySpark (Structured Streaming + Kafka)</h1>\n<p>Start and create Kafka Topic:</p>\n<pre><code># KAFKA\n\n1. $KAFKA_HOME/bin/zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties (Start Zookeeper)\n2. $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_HOME/config/server.properties (Start Kafka Server)\nIn GRIFFON skip step 1 and 2 and simply go to Applications &gt; Big Data &gt; Start Kafka\n3. $KAFKA_HOME/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic testlogs (Create example topic &quot;testlogs&quot;)\ncd applied-ds\n4. $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic testlogs &lt; data/stream/apache-access.log (Initialize Kafka Producer)\n</code></pre>\n<pre><code># Spark\n\n5. Spark subscribe to Kafka topic using the Structured Streaming API (Kafka Consumer)\n6. Spark DataFrame manipulations of incoming stream of messages\n7. Spark Sink out (various options)\n</code></pre>\n<p>Docs:</p>\n<ul>\n  <li>Kafka Quick Start (<a href=\"https://kafka.apache.org/quickstart\">https://kafka.apache.org/quickstart</a>)</li>\n  <li>Spark Structured Streaming in micro-batches (<a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html</a>)</li>\n  <li>Kafka Integration (<a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\">http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html</a>)</li>\n  <li>Zeppelin interpreter settings (<a href=\"https://zeppelin.apache.org/docs/0.5.0-incubating/interpreter/spark.html\">https://zeppelin.apache.org/docs/0.5.0-incubating/interpreter/spark.html</a>)</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1499907490828_-200030489","id":"20170708-181643_892778384","dateCreated":"2017-07-13T01:58:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9137","user":"anonymous","dateFinished":"2017-07-13T02:02:32+0100","dateStarted":"2017-07-13T02:02:32+0100"},{"text":"%pyspark\n\n# Version 2: Parsing Apache Logs via DataFame manipulations\nregex_pattern = '^(\\S+) (\\S+) (\\S+) \\[([\\w:/]+\\s[+\\-]\\d{4})\\] \"(\\S+) (\\S+) (\\S+)\" (\\d{3}) (\\d+)'\ndef parse_apache_log(df_raw):\n\n    df = df_raw.select(regexp_extract('value', regex_pattern, 1).alias('Host'),\n                       regexp_extract('value', regex_pattern, 2).alias('ClientID'),\n                       regexp_extract('value', regex_pattern, 3).alias('UserID'),\n                       regexp_extract('value', regex_pattern, 4).alias('DateTime'),\n                       regexp_extract('value', regex_pattern, 5).alias('Method'),\n                       regexp_extract('value', regex_pattern, 6).alias('Endpoint'),\n                       regexp_extract('value', regex_pattern, 7).alias('Protocol'),\n                       regexp_extract('value', regex_pattern, 8).alias('Response').cast(IntegerType()),\n                       regexp_extract('value', regex_pattern, 9).alias('SizeBytes').cast(FloatType()))\n\n    df = df.select([c for c in df.columns if c not in ['ClientID', 'UserID', 'Method']])\n    # print('Length of DataFrame: ', df.count())\n    return df","dateUpdated":"2017-07-13T02:02:32+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1499907490828_-200030489","id":"20170708-223438_1543629318","dateCreated":"2017-07-13T01:58:10+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9138","user":"anonymous","dateFinished":"2017-07-13T02:02:33+0100","dateStarted":"2017-07-13T02:02:32+0100"},{"text":"%md\n\n## Run main\\_pyspark\\_streaming\\_apachelogs\\_kafka.py from terminal instead\n\n```spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.1 main_pyspark_streaming_apachelogs_kafka.py```\n\nEasier since output gets printed to STDOUT and can go on for infinity...","user":"anonymous","dateUpdated":"2017-07-13T02:05:34+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1499907797905_1076210617","id":"20170713-020317_1356859774","dateCreated":"2017-07-13T02:03:17+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9780","dateFinished":"2017-07-13T02:05:34+0100","dateStarted":"2017-07-13T02:05:34+0100","results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Run main_pyspark_streaming_apachelogs_kafka.py from terminal instead</h2>\n<p><code>spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.1 main_pyspark_streaming_apachelogs_kafka.py</code></p>\n<p>Easier since output gets printed to STDOUT and can go on for infinity&hellip;</p>\n</div>"}]}},{"text":"%pyspark\n\n\nwhile True:\n    print(\"Kafka streaming running\")\n    ds0 = spark \\\n        .readStream \\\n        .format(\"kafka\") \\\n        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n        .option(\"subscribe\", \"testlogs\") \\\n        .load()\n\n    # incoming values are in bytes, the next line deserializes the bytes and regular df operations can be applied\n    ds0 = ds0.selectExpr(\"CAST(value AS STRING)\", \"CAST(timestamp AS STRING)\")\n    ds1 = parse_apache_log(ds0.select(\"value\"))\n\n    query = ds1.writeStream \\\n        .format(\"console\") \\\n        .start()\n        \n\n    query.awaitTermination()","dateUpdated":"2017-07-13T02:03:24+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1499907490828_-200030489","id":"20170708-183142_1294411996","dateCreated":"2017-07-13T01:58:10+0100","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9139","user":"anonymous","dateFinished":"2017-07-13T02:03:02+0100","dateStarted":"2017-07-13T02:02:33+0100","focus":true},{"text":"%pyspark\n","dateUpdated":"2017-07-13T02:02:32+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1499907490829_-200415238","id":"20170708-184637_58063982","dateCreated":"2017-07-13T01:58:10+0100","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:9140","user":"anonymous"}],"name":"Worksheet 11 - Zeppelin Part2 - PySpark + ELK + Kafka -Answers","id":"2CPMUBA7C","angularObjects":{"2CG454D1G:shared_process":[],"2CJM3398J:shared_process":[],"2CHKBQDC1:shared_process":[],"2CKR239V2:shared_process":[],"2CMH77894:shared_process":[],"2CG3KGRMA:shared_process":[],"2CGJKKWKC:shared_process":[],"2CJ8M92Q1:shared_process":[],"2CJ85YN4D:shared_process":[],"2CGN16NVF:shared_process":[],"2CJ2JVKJZ:shared_process":[],"2CGC1GUA5:shared_process":[],"2CHX4RVKW:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}