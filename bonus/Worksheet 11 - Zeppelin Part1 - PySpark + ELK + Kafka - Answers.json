{"paragraphs":[{"text":"%pyspark\n# PySpark\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\n# Python\nimport json\nimport pandas as pd\nimport numpy as np\nimport re, sys, os\nfrom datetime import datetime\n","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1499897989261_63093733","id":"20170707-231550_2002966558","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:30+0100","dateFinished":"2017-07-13T01:50:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:7865"},{"text":"%md\n# PCAP PySpark and ElasticSearch Connector\n\nData Sorce: https://www.netresec.com/?page=MACCDC. tshark command to parse pcap and save to csv below.\n\n```\ntshark -r maccdc2012_00000.pcap.gz -e frame.number -e frame.time -e frame.protocols -e frame.len -e tcp.len -e tcp.hdr_len -e ip.src -e ip.dst -e tcp.srcport -e tcp.dstport -e tcp.flags -e ip.geoip.country -e ip.geoip.city -e ip.geoip.lat -e ip.geoip.lon -E separator='|' -Tfields > maccdc2012_00000.csv\n```\n\nThis spark code needs an additional library installed (```elasticsearch-hadoop-5.5.0.jar``` to write and read to and from an ElasticSearch 5.5 cluster). It can simply be installed via copying and pasting the ```jar``` file into the ```jars``` folder within the ```SPARK_HOME``` directory. \n\n\n- ElasticSearch Hadoop docs (https://www.elastic.co/guide/en/elasticsearch/hadoop/5.4/spark.html)\n- Additional docs (https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html)\n- Databricks docs (https://docs.databricks.com/spark/latest/data-sources/elasticsearch.html)\n- Download latest elasticsearch-hadoop jar corresponding to your ElasticSearch installation (https://jar-download.com/?search_box=elasticsearch-hadoop)\n","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>PCAP PySpark and ElasticSearch Connector</h1>\n<p>Data Sorce: <a href=\"https://www.netresec.com/?page=MACCDC\">https://www.netresec.com/?page=MACCDC</a>. tshark command to parse pcap and save to csv below.</p>\n<pre><code>tshark -r maccdc2012_00000.pcap.gz -e frame.number -e frame.time -e frame.protocols -e frame.len -e tcp.len -e tcp.hdr_len -e ip.src -e ip.dst -e tcp.srcport -e tcp.dstport -e tcp.flags -e ip.geoip.country -e ip.geoip.city -e ip.geoip.lat -e ip.geoip.lon -E separator=&#39;|&#39; -Tfields &gt; maccdc2012_00000.csv\n</code></pre>\n<p>This spark code needs an additional library installed (<code>elasticsearch-hadoop-5.5.0.jar</code> to write and read to and from an ElasticSearch 5.5 cluster). It can simply be installed via copying and pasting the <code>jar</code> file into the <code>jars</code> folder within the <code>SPARK_HOME</code> directory. </p>\n<ul>\n  <li>ElasticSearch Hadoop docs (<a href=\"https://www.elastic.co/guide/en/elasticsearch/hadoop/5.4/spark.html\">https://www.elastic.co/guide/en/elasticsearch/hadoop/5.4/spark.html</a>)</li>\n  <li>Additional docs (<a href=\"https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html\">https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html</a>)</li>\n  <li>Databricks docs (<a href=\"https://docs.databricks.com/spark/latest/data-sources/elasticsearch.html\">https://docs.databricks.com/spark/latest/data-sources/elasticsearch.html</a>)</li>\n  <li>Download latest elasticsearch-hadoop jar corresponding to your ElasticSearch installation (<a href=\"https://jar-download.com/?search_box=elasticsearch-hadoop\">https://jar-download.com/?search_box=elasticsearch-hadoop</a>)</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1499897989261_63093733","id":"20170708-003405_1895259614","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:30+0100","dateFinished":"2017-07-13T01:50:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7866"},{"text":"%pyspark\n\nDATA_HOME = '/home/griffonuser/applied-ds/data/'\n\nfname = DATA_HOME + 'maccdc2012_00000.csv.gz'\n\ndf = sqlContext.read\\\n    .format('com.databricks.spark.csv')\\\n    .options(delimiter=\"|\", header=\"false\", inferSchema=\"true\")\\\n    .load(fname)\n    \ncolNames = [\"frameNumber\", \"frameTime\", \"frameProtocols\", \"frameLen\", \"tcpLen\", \"tcpHdrlen\",\n    \"ipSrc\", \"ipDst\", \"tcpSrcport\", \"tcpDstport\", \"tcpFlags\", \"country\", \"city\", \"lat\", \"lon\"]\n\nmapping = dict(zip(['_c'+str(i) for i in range(len(df.columns))], colNames))\ndf = df.select([col(c).alias(mapping.get(c, c)) for c in df.columns])\ndf = df.na.drop(how='any')\n# df.printSchema()\n\nselectedCols = [\"frameTime\", \"ipSrc\", \"ipDst\", \"tcpSrcport\", \"tcpDstport\", \"frameLen\", \"lat\", \"lon\"]\ndf = df.select(selectedCols)  # or select specific cols only\ndf.show()\ndf.printSchema()\nimport sys\nprint(sys.version)\n","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------+--------------+----------+----------+--------+---------+-----------+\n|           frameTime|         ipSrc|         ipDst|tcpSrcport|tcpDstport|frameLen|      lat|        lon|\n+--------------------+--------------+--------------+----------+----------+--------+---------+-----------+\n|Mar 16, 2012 08:3...|192.168.202.76|   64.4.23.149|     51655|     40006|      70|37.339401|-121.894997|\n|Mar 16, 2012 08:3...|192.168.202.76|  157.56.52.15|     51658|     40046|      70|47.680099|-122.120598|\n|Mar 16, 2012 08:3...|192.168.202.76|   64.4.23.149|     51659|        80|      70|37.339401|-121.894997|\n|Mar 16, 2012 08:3...|192.168.202.76| 157.55.56.150|     51660|       443|      70|41.849998| -87.650002|\n|Mar 16, 2012 08:3...|192.168.202.76|157.55.235.162|     51661|     40030|      70|53.333099|    -6.2489|\n|Mar 16, 2012 08:3...|192.168.202.76|   64.4.23.149|     51656|       443|      70|37.339401|-121.894997|\n|Mar 16, 2012 08:3...|192.168.202.76|  157.56.52.15|     51663|       443|      70|47.680099|-122.120598|\n|Mar 16, 2012 08:3...|192.168.202.76| 157.55.56.150|     51657|     40024|      70|41.849998| -87.650002|\n|Mar 16, 2012 08:3...|192.168.202.76| 157.55.56.150|     51664|        80|      70|41.849998| -87.650002|\n|Mar 16, 2012 08:3...|192.168.202.76|157.55.235.162|     51665|       443|      70|53.333099|    -6.2489|\n|Mar 16, 2012 08:3...|192.168.202.76|  157.56.52.15|     51658|     40046|      70|47.680099|-122.120598|\n|Mar 16, 2012 08:3...|192.168.202.76|   64.4.23.149|     51659|        80|      70|37.339401|-121.894997|\n|Mar 16, 2012 08:3...|192.168.202.76|  157.56.52.15|     51666|        80|      70|47.680099|-122.120598|\n|Mar 16, 2012 08:3...|192.168.202.76| 157.55.56.150|     51660|       443|      70|41.849998| -87.650002|\n|Mar 16, 2012 08:3...|192.168.202.76|157.55.235.162|     51661|     40030|      70|53.333099|    -6.2489|\n|Mar 16, 2012 08:3...|192.168.202.76|157.55.235.162|     51667|        80|      70|53.333099|    -6.2489|\n|Mar 16, 2012 08:3...|192.168.202.76|  157.56.52.15|     51663|       443|      70|47.680099|-122.120598|\n|Mar 16, 2012 08:3...|192.168.202.76| 157.55.56.150|     51664|        80|      70|41.849998| -87.650002|\n|Mar 16, 2012 08:3...|192.168.202.76|157.55.235.162|     51665|       443|      70|53.333099|    -6.2489|\n|Mar 16, 2012 08:3...|192.168.202.76|   64.4.23.149|     51655|     40006|      66|37.339401|-121.894997|\n+--------------------+--------------+--------------+----------+----------+--------+---------+-----------+\nonly showing top 20 rows\n\nroot\n |-- frameTime: string (nullable = true)\n |-- ipSrc: string (nullable = true)\n |-- ipDst: string (nullable = true)\n |-- tcpSrcport: integer (nullable = true)\n |-- tcpDstport: integer (nullable = true)\n |-- frameLen: integer (nullable = true)\n |-- lat: double (nullable = true)\n |-- lon: double (nullable = true)\n\n3.5.3 |Anaconda custom (64-bit)| (default, Mar  6 2017, 11:58:13) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n"}]},"apps":[],"jobName":"paragraph_1499897989261_63093733","id":"20170707-232824_1015708829","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:30+0100","dateFinished":"2017-07-13T01:50:34+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7867"},{"text":"%pyspark\ndef datetime_iso8601(x):\n\n    date = datetime.strptime(str(x), '%b %d, %Y %H:%M:%S.%f000 EDT')\n    date = date.isoformat()\n    string = str(date)\n    return string\n\ndef join_location(x, y):\n    string = str(x) + \",\" + str(y)\n    return string\n\ntoES_datetime = udf(lambda x: datetime_iso8601(str(x)), StringType())\nto_datetime = udf(lambda x: datetime.strptime(str(x), '%b %d, %Y %H:%M:%S.%f000 EDT'), TimestampType())\ntoLocation_str = udf(lambda x, y: join_location(x, y), StringType())\n\ndf = df.withColumn(\"@timestamp\", toES_datetime(col(\"frameTime\")))\\\n        .withColumn(\"location\", toLocation_str(col(\"lat\"), col(\"lon\")))\\\n        .withColumn(\"frameTime\", to_datetime(col(\"frameTime\")))\ndf1 = df\n\ndf1 = df1.select([\"ipSrc\", \"ipDst\", \"tcpSrcport\", \"tcpDstport\", \"@timestamp\", \"location\", \"frameLen\"])\n\nprint(df1.count())\ndf1.cache()  # save/cache, don't recompute\ndf1.show()\ndf1.printSchema()","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2661\n+--------------+--------------+----------+----------+--------------------+--------------------+--------+\n|         ipSrc|         ipDst|tcpSrcport|tcpDstport|          @timestamp|            location|frameLen|\n+--------------+--------------+----------+----------+--------------------+--------------------+--------+\n|192.168.202.76|   64.4.23.149|     51655|     40006|2012-03-16T08:30:...|37.339401,-121.89...|      70|\n|192.168.202.76|  157.56.52.15|     51658|     40046|2012-03-16T08:30:...|47.680099,-122.12...|      70|\n|192.168.202.76|   64.4.23.149|     51659|        80|2012-03-16T08:30:...|37.339401,-121.89...|      70|\n|192.168.202.76| 157.55.56.150|     51660|       443|2012-03-16T08:30:...|41.849998,-87.650002|      70|\n|192.168.202.76|157.55.235.162|     51661|     40030|2012-03-16T08:30:...|   53.333099,-6.2489|      70|\n|192.168.202.76|   64.4.23.149|     51656|       443|2012-03-16T08:30:...|37.339401,-121.89...|      70|\n|192.168.202.76|  157.56.52.15|     51663|       443|2012-03-16T08:30:...|47.680099,-122.12...|      70|\n|192.168.202.76| 157.55.56.150|     51657|     40024|2012-03-16T08:30:...|41.849998,-87.650002|      70|\n|192.168.202.76| 157.55.56.150|     51664|        80|2012-03-16T08:30:...|41.849998,-87.650002|      70|\n|192.168.202.76|157.55.235.162|     51665|       443|2012-03-16T08:30:...|   53.333099,-6.2489|      70|\n|192.168.202.76|  157.56.52.15|     51658|     40046|2012-03-16T08:30:...|47.680099,-122.12...|      70|\n|192.168.202.76|   64.4.23.149|     51659|        80|2012-03-16T08:30:...|37.339401,-121.89...|      70|\n|192.168.202.76|  157.56.52.15|     51666|        80|2012-03-16T08:30:...|47.680099,-122.12...|      70|\n|192.168.202.76| 157.55.56.150|     51660|       443|2012-03-16T08:30:...|41.849998,-87.650002|      70|\n|192.168.202.76|157.55.235.162|     51661|     40030|2012-03-16T08:30:...|   53.333099,-6.2489|      70|\n|192.168.202.76|157.55.235.162|     51667|        80|2012-03-16T08:30:...|   53.333099,-6.2489|      70|\n|192.168.202.76|  157.56.52.15|     51663|       443|2012-03-16T08:30:...|47.680099,-122.12...|      70|\n|192.168.202.76| 157.55.56.150|     51664|        80|2012-03-16T08:30:...|41.849998,-87.650002|      70|\n|192.168.202.76|157.55.235.162|     51665|       443|2012-03-16T08:30:...|   53.333099,-6.2489|      70|\n|192.168.202.76|   64.4.23.149|     51655|     40006|2012-03-16T08:30:...|37.339401,-121.89...|      66|\n+--------------+--------------+----------+----------+--------------------+--------------------+--------+\nonly showing top 20 rows\n\nroot\n |-- ipSrc: string (nullable = true)\n |-- ipDst: string (nullable = true)\n |-- tcpSrcport: integer (nullable = true)\n |-- tcpDstport: integer (nullable = true)\n |-- @timestamp: string (nullable = true)\n |-- location: string (nullable = true)\n |-- frameLen: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1499897989262_64247980","id":"20170708-002545_1975746750","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:30+0100","dateFinished":"2017-07-13T01:50:41+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7868"},{"text":"%md\n```\n# In Kibana Dev Tools run the following mapping first:\n\nPUT _template/template_1\n{\n  \"template\": \"bh*\",\n  \"settings\": {\n    \"mapping.ignore_malformed\":true,\n    \"number_of_shards\": 1\n  },\n  \"mappings\": {\n    \"pcap\": {\n      \"_source\": {\n        \"enabled\": true\n      },\n      \"properties\": {\n        \"location\": {\n          \"type\": \"geo_point\"\n        },\n        \"@timestamp\": {\n          \"type\": \"date\"\n        },\n        \"ipSrc\": {\n          \"type\": \"text\"\n        },\n        \"ipDst\": {\n          \"type\": \"text\"\n        },\n        \"tcpSrcport\": {\n          \"type\": \"integer\"\n        },\n        \"tcpDstport\": {\n          \"type\": \"integer\"\n        },\n        \"frameLen\": {\n          \"type\": \"integer\"\n        }\n      }\n    }\n  }\n}\n```","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<pre><code># In Kibana Dev Tools run the following mapping first:\n\nPUT _template/template_1\n{\n  &quot;template&quot;: &quot;bh*&quot;,\n  &quot;settings&quot;: {\n    &quot;mapping.ignore_malformed&quot;:true,\n    &quot;number_of_shards&quot;: 1\n  },\n  &quot;mappings&quot;: {\n    &quot;pcap&quot;: {\n      &quot;_source&quot;: {\n        &quot;enabled&quot;: true\n      },\n      &quot;properties&quot;: {\n        &quot;location&quot;: {\n          &quot;type&quot;: &quot;geo_point&quot;\n        },\n        &quot;@timestamp&quot;: {\n          &quot;type&quot;: &quot;date&quot;\n        },\n        &quot;ipSrc&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;ipDst&quot;: {\n          &quot;type&quot;: &quot;text&quot;\n        },\n        &quot;tcpSrcport&quot;: {\n          &quot;type&quot;: &quot;integer&quot;\n        },\n        &quot;tcpDstport&quot;: {\n          &quot;type&quot;: &quot;integer&quot;\n        },\n        &quot;frameLen&quot;: {\n          &quot;type&quot;: &quot;integer&quot;\n        }\n      }\n    }\n  }\n}\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1499897989263_63863231","id":"20170708-132042_504100134","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:30+0100","dateFinished":"2017-07-13T01:50:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7869"},{"text":"%pyspark\n\n# Sink out to ElasticSearch\nindex_name = \"bh-2017.07.08/pcap\"\n\n# Note: requires elasticsearch-hadoop jar corresponding \n# to your ElasticSearch installation (https://jar-download.com/?search_box=elasticsearch-hadoop)\n\ndf1.write.format(\"org.elasticsearch.spark.sql\").mode(\"append\")\\\n    .option(\"es.resource\", index_name)\\\n    .option(\"es.index.auto.create\", \"true\")\\\n    .option(\"es.mapping.date.rich\", \"true\")\\\n    .option(\"es.nodes.wan.only\",\"false\")\\\n    .option(\"es.net.ssl\",\"false\")\\\n    .option(\"es.nodes\", \"localhost\")\\\n    .option(\"es.port\", \"9200\")\\\n    .save()\n    \nprint(\"Successfully wrote df to ElasticSearch\")","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Successfully wrote df to ElasticSearch\n"}]},"apps":[],"jobName":"paragraph_1499897989263_63863231","id":"20170708-002554_1039269040","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:34+0100","dateFinished":"2017-07-13T01:50:43+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7870"},{"text":"%pyspark\n\n# Read from ElasticSearch\nindex_name = \"bh-2017.07.08/pcap\"\n\ndf_from_es = sqlContext.read.format(\"org.elasticsearch.spark.sql\")\\\n    .option(\"es.resource\", index_name)\\\n    .option(\"es.nodes\" , \"localhost\")\\\n    .option(\"es.port\",\"9200\")\\\n    .option(\"es.nodes.wan.only\",\"true\")\\\n    .load()\n\ndf_from_es.printSchema()\n\n","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- @timestamp: timestamp (nullable = true)\n |-- frameLen: integer (nullable = true)\n |-- ipDst: string (nullable = true)\n |-- ipSrc: string (nullable = true)\n |-- location: string (nullable = true)\n |-- tcpDstport: integer (nullable = true)\n |-- tcpSrcport: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1499897989264_74251452","id":"20170708-145540_361424214","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:42+0100","dateFinished":"2017-07-13T01:50:43+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7871"},{"text":"%md\n\n# PCAP PySpark ETL (groupby, time windows and agg)\n","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>PCAP PySpark ETL (groupby, time windows and agg)</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1499897989264_74251452","id":"20170708-131449_660580866","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:30+0100","dateFinished":"2017-07-13T01:50:30+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7872"},{"text":"%pyspark\nexprs = [sum(\"frameLen\").alias(\"frameLenSum\"), count(\"ipSrc\").alias(\"NPackets\")]\n\ndf_agg = df.groupBy([window(\"frameTime\", \"5 seconds\"), \"ipSrc\", \"ipDst\"])\\\n     .agg(*exprs).orderBy(\"window.start\").na.drop(how=\"any\")\n     \ndf_agg.show()\ndf_agg.count()\n# df_agg.createOrReplaceTempView(\"features\")","user":"anonymous","dateUpdated":"2017-07-13T01:50:30+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------+--------------+--------------+-----------+--------+\n|              window|         ipSrc|         ipDst|frameLenSum|NPackets|\n+--------------------+--------------+--------------+-----------+--------+\n|[2012-03-16 08:30...|192.168.202.76|157.55.235.162|        280|       4|\n|[2012-03-16 08:30...|192.168.202.76|   64.4.23.149|        280|       4|\n|[2012-03-16 08:30...|192.168.202.76|  157.56.52.15|        350|       5|\n|[2012-03-16 08:30...|192.168.202.76| 157.55.56.150|        280|       4|\n|[2012-03-16 08:30...|192.168.202.76|   64.4.23.149|        198|       3|\n|[2012-03-16 08:30...|192.168.202.76|  157.56.52.24|         70|       1|\n|[2012-03-16 08:30...|192.168.202.76|157.55.235.162|        140|       2|\n|[2012-03-16 08:30...|192.168.202.76|  157.56.52.15|        136|       2|\n|[2012-03-16 08:30...|192.168.202.76| 157.55.56.150|        202|       3|\n|[2012-03-16 08:30...|192.168.202.76| 157.55.56.160|        140|       2|\n|[2012-03-16 08:30...|192.168.202.76|157.55.235.162|        198|       3|\n|[2012-03-16 08:30...|192.168.202.76|  157.56.52.15|        132|       2|\n|[2012-03-16 08:30...|192.168.202.76|  157.56.52.24|        350|       5|\n|[2012-03-16 08:30...|192.168.202.76| 157.55.56.150|         66|       1|\n|[2012-03-16 08:30...|192.168.202.76| 157.55.56.160|        280|       4|\n|[2012-03-16 08:30...|192.168.202.76| 157.55.56.160|        198|       3|\n|[2012-03-16 08:30...|192.168.202.76|  157.56.52.24|        132|       2|\n|[2012-03-16 08:30...|192.168.202.76|  157.56.52.24|         66|       1|\n|[2012-03-16 08:30...|192.168.202.76|157.55.130.165|        140|       2|\n|[2012-03-16 08:30...|192.168.202.76|111.221.77.148|         70|       1|\n+--------------------+--------------+--------------+-----------+--------+\nonly showing top 20 rows\n\n1092\n"}]},"apps":[],"jobName":"paragraph_1499897989264_74251452","id":"20170708-110633_294921180","dateCreated":"2017-07-12T23:19:49+0100","dateStarted":"2017-07-13T01:50:43+0100","dateFinished":"2017-07-13T01:50:55+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:7873"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2017-07-12T23:22:08+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1499897989267_74636200","id":"20170708-162314_417707524","dateCreated":"2017-07-12T23:19:49+0100","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:7874"}],"name":"Worksheet 11 - Zeppelin Part1 - PySpark + ELK + Kafka - Answers","id":"2CP5RP2KV","angularObjects":{"2CG454D1G:shared_process":[],"2CJM3398J:shared_process":[],"2CHKBQDC1:shared_process":[],"2CKR239V2:shared_process":[],"2CMH77894:shared_process":[],"2CG3KGRMA:shared_process":[],"2CGJKKWKC:shared_process":[],"2CJ8M92Q1:shared_process":[],"2CJ85YN4D:shared_process":[],"2CGN16NVF:shared_process":[],"2CJ2JVKJZ:shared_process":[],"2CGC1GUA5:shared_process":[],"2CHX4RVKW:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}