{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/logo_white_bkg_small.png\" align=\"left\" />\n",
    "\n",
    "# Worksheet 6.0 Clustering\n",
    "This worksheet covers concepts relating to unsupervised learning.  It should take no more than 20-30 minutes to complete.  Please raise your hand if you get stuck.  \n",
    "\n",
    "There are many ways to accomplish the tasks that you are presented with, however you will find that by using the techniques covered in class, the exercises should be relatively simple. \n",
    "\n",
    "## Import the Libraries\n",
    "For this exercise, we will be using:\n",
    "* Pandas (http://pandas.pydata.org/pandas-docs/stable/)\n",
    "* Numpy (https://docs.scipy.org/doc/numpy/reference/)\n",
    "* Matplotlib (http://matplotlib.org/api/pyplot_api.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from scipy.spatial.distance import cdist\n",
    "style.use(\"ggplot\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "In this example notebook, you will see how to implement K-Means Clustering in Python using Scikit-Learn and Pandas. \n",
    "Adapted from https://pythonprogramming.net/flat-clustering-machine-learning-python-scikit-learn/\n",
    "\n",
    "## Step 1:  Get Data:\n",
    "The first step is to prepare or generate the data.  In this dataset, the observations only have two features, but K-Means can be used with any number of features.  Since this is an unsupervised example, it is not necessary to have a \"target\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame([[1, 2],\n",
    "              [5, 8],\n",
    "              [1.5, 1.8],\n",
    "              [8, 8],\n",
    "              [1, 0.6],\n",
    "              [9, 11]], columns=['x','y'])\n",
    "print( data )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2:  Build the Model:\n",
    "Much like the supervised models, you first create the model then call the `.fit()` method using your data source.  The model is now populated with both your centroids and labels.  These can be accessed via the `.cluster_centers_` and `labels_` properties respectively.\n",
    "\n",
    "You can view the complete documentation here: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "\n",
    "K-Means also has a `.predict()` method which can be used to predict the label for an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2).fit(data)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "labels = kmeans.labels_\n",
    "\n",
    "print(centroids)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.DataFrame([[4,2]])\n",
    "\n",
    "kmeans.predict(test)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the Clusters\n",
    "The code below visualizes the clusters.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['labels'] = labels\n",
    "\n",
    "#plt.plot(data, colors[data['labels'], markersize = 10)\n",
    "\n",
    "group1 = data[data['labels']==1].plot( kind='scatter', x='x', y='y',  s=100, color='DarkGreen', label=\"Group 1\" )\n",
    "group2 = data[data['labels']==0].plot( kind='scatter', x='x', y='y', s=100,color='Brown', ax=group1, label=\"Group 2\" )\n",
    "group1.legend(loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "          ncol=3, fancybox=True, shadow=True)\n",
    "plt.scatter(centroids[:, 0],centroids[:, 1], marker = \"x\", s=150, linewidths = 5, zorder = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with Real Data\n",
    "Now that you've tried K-means on some generated data, let's try it on some real data and see what we can produce. As before the first step is to read in the data into a DataFrame.  \n",
    "\n",
    "We will be using this data later, but the dataset consists of approximately 6000 domains--5000 of which were generated by various botnets and 1000 are from the Alexa 1 Million.  The columns are:\n",
    "\n",
    "* `dsrc`:  The source of the domain\n",
    "* `domain`:  The actual domain\n",
    "* `length`:  The length of the domain\n",
    "* `dicts`:  Percentage containing dictionary words\n",
    "* `entropy`:  The entropy of the domain\n",
    "* `numbers`:  The number of digits in the domain\n",
    "* `ngram`:  Different n-grams which appear in the domain (?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/dga-full.csv')\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the Data\n",
    "Since clustering relies on measuring distances between objects it is important that all data points be on the same scale.  There are various methods for doing this, which are beyond the scope of this class, however, for this example, we will use scikit-learn's `StandardScaler` to accomplish this.  (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)\n",
    "\n",
    "The StandardScaler transforms each column by:\n",
    "* Subtracting from the element in each row the mean for each feature (column) and then taking this value and\n",
    "* Dividing by that feature's (column's) standard deviation.\n",
    "\n",
    "Scikit-learn has a transformer interface which is very similar to the other scikit-learn interfaces.  The basic steps are:\n",
    "1.  Create the Scaler object\n",
    "2.  Using the feature matrix, call the `.fit()` method to \"train\" the Scaler\n",
    "3.  Use the `.transform()` method to scale the data.\n",
    "\n",
    "**NOTE**: When using a Scaler, it is important to train the scaler on your data, and use this trained scalers on any future predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['length', 'dicts','entropy','numbers','ngram']\n",
    "scaled_feature_columns = ['scaled_length', 'scaled_dicts','scaled_entropy','scaled_numbers','scaled_ngram']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1:  Create the scaler\n",
    "\n",
    "\n",
    "#Steps 2 & 3:  Fit the scaler and transform this data\n",
    "\n",
    "\n",
    "#Put the scaled data into a dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the data and you'll see that the data is now all scaled consistently. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally for convenience, we're going to merge the scaled data with the non-scaled data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.merge( data, scaled_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn!\n",
    "Now that we have data that is suitable (maybe) for clustering, in the section below, perform K-Means clustering on this data set.  Initially, start out with 2 clusters and assign the `cluster id` as a column in your DataFrame.\n",
    "\n",
    "Then do a `value_counts()` on the `dsrc` column for each cluster to see how the model divided the data.  Try various values for `k` to see how it performed.\n",
    "\n",
    "Remember to use the **scaled features** for your clustering.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Food for thought:\n",
    "Now that you've done clustering with various numbers of clusters, it appears that the data acutally does break evenly into 2 clusters.  Take a look at the original data and see if you can come up with a reason why that is the case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Visualizing Performance\n",
    "As we already know, it is difficult to measure the performance of clustering models since there usually is no known ground truth from which to evaluate your model.  However, there are two techniques which \n",
    "\n",
    "The K-Elbow Visualizer implements the “elbow” method of selecting the optimal number of clusters for K-means clustering. K-means is a simple unsupervised machine learning algorithm that groups data into a specified number (k) of clusters. Because the user must specify in advance what k to choose, the algorithm is somewhat naive – it assigns all members to k clusters even if that is not the right k for the dataset.\n",
    "\n",
    "The elbow method runs k-means clustering on the dataset for a range of values for k (say from 1-10) and then for each value of k computes an average score for all clusters. By default, the distortion_score is computed, the sum of square distances from each point to its assigned center. Other metrics can also be used such as the silhouette_score, the mean silhouette coefficient for all samples or the calinski_harabaz_score, which computes the ratio of dispersion between and within clusters.\n",
    "\n",
    "When these overall metrics for each model are plotted, it is possible to visually determine the best value for K. If the line chart looks like an arm, then the “elbow” (the point of inflection on the curve) is the best value of k. The “arm” can be either up or down, but if there is a strong inflection point, it is a good indication that the underlying model fits best at that point. (http://www.scikit-yb.org/en/latest/api/cluster/elbow.html)\n",
    "\n",
    "In python there is a module called `YellowBrick` which facilitates visualizing the K-Elbow score.  All of YellowBrick's visualizations follow essentually the same pattern:\n",
    "\n",
    "1.  Create the Visualizer Object\n",
    "2.  Call the `.fit()` method using the data\n",
    "3.  Call the `.poof()` method to render the visualization\n",
    "\n",
    "The snippet below demonstrates how to use the elbow method to visualize the clustering model's performance on this dataset.\n",
    "```python\n",
    "visualizer = KElbowVisualizer(KMeans(), k=(4,12))\n",
    "\n",
    "visualizer.fit( feature_matrix ) \n",
    "visualizer.poof()\n",
    "```\n",
    "\n",
    "### Your Turn!\n",
    "In the box below, create a visualization using the elbow method to see if there are any inflection points in the distortion score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Way to Visualize Clustering Performance\n",
    "The Silhouette Coefficient is used when the ground-truth about the dataset is unknown and computes the density of clusters computed by the model. The score is computed by averaging the silhouette coefficient for each sample, computed as the difference between the average intra-cluster distance and the mean nearest-cluster distance for each sample, normalized by the maximum value. This produces a score between 1 and -1, where 1 is highly dense clusters and -1 is completely incorrect clustering. (http://www.scikit-yb.org/en/latest/api/cluster/silhouette.html)\n",
    "\n",
    "\n",
    "### Your Turn!\n",
    "Using the YellowBrick `SilhouetteVisualizer`, try visualizing models with various values of `K`.\n",
    "\n",
    "**Note**:  This visualization is quite expensive, so I recommend performing this using a sample o your original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DBSCAN\n",
    "Now that you've tried K-Means, let's try doing some clustering using DBSCAN (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  Remember that the main tuning parameters for DBSCAN are:\n",
    "\n",
    "* **epsilon (eps)**:  The minimum distance between two samples \n",
    "* **min_samples**:  The minimum number of samples needed to form a neighborhood\n",
    "\n",
    "By default epsilon is 0.5 and the min_samples is 5. First, try DBSCAN with the default options.  If you use the `fit_predict()` function, you can save the results in a new column in your data.  \n",
    "\n",
    "How did this compare with K-Means?  Given that you actually know what the data really is, how did DBSCAN do in terms of identifing meaningful clusters?  Look at the `dsrc` column and do `value_counts()` for the various neighhborhoods.  What did you notice?\n",
    "\n",
    "Try again, but this time experiment with the values of epsilon and min_samples and see what DBSCAN comes up with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
