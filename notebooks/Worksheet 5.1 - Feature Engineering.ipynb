{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime \n",
    "from collections import Counter\n",
    "from sklearn import feature_extraction, tree, model_selection, metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "import seaborn as sns\n",
    "from yellowbrick.features.rankd import Rank2D\n",
    "from yellowbrick.features.radviz import RadViz\n",
    "from yellowbrick.features.pcoords import ParallelCoordinates\n",
    "from yellowbrick.features import JointPlotVisualizer\n",
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import entropy\n",
    "%matplotlib inline\n",
    "\n",
    "# Useful cyber libraries\n",
    "import whois   # pip install python-whois\n",
    "import tldextract  # pip install tldextract \n",
    "import ipaddress  # pip install ipaddress\n",
    "import dns.query  # pip install dnspython\n",
    "import dns.resolver\n",
    "from dns.exception import DNSException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/logo_white_bkg_small.png\" align=\"right\" />   \n",
    "\n",
    "\n",
    "## Worksheet 5.1 - Feature Engineering: Malicious URL Detection using Machine Learning\n",
    "\n",
    "This worksheet is a step-by-step guide on how to train a Machine Learning model that can detect malicious URLs. We will walk you through the process of transforming raw URL strings to Machine Learning features and creating a Decision Tree Classifer which you will use to determine whether a given URL is malicious or not. Once you have implemented the classifier, the worksheet will walk you through evaluating your model.  \n",
    "\n",
    "### Overview 2 main steps:\n",
    "\n",
    "1. **Feature Engineering** - from raw URL strings to features using [pandas](http://pandas.pydata.org/pandas-docs/stable/) DataFrame, datetime and [numpy](http://www.numpy.org/) manipulations.\n",
    "2. **Machine Learning Classification** - predict whether a URL is malicious or not using a Decision Tree Classifier in [sklearn](http://scikit-learn.org/stable/) and evaluate model performance\n",
    "\n",
    "We provide an additional notebook where you can see how to use \"Featureless Deep Learning\" to build such a classifier.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "The dataset was build from various different open source data sources. Computationally intensive tasks such as retrieving the creation time for each unique domain in the data set via [whois](https://pypi.python.org/pypi/python-whois) have already been performed beforehand. Some of the open source URLs came with the zone apex only, others didn't include the protocol, therefore, we uniformly removed the protocol (http:// or https://) and subdomain (e.g. www) from the URL string if applicable.\n",
    "\n",
    "Benign\n",
    "- Custom automated webscraping of [Alexa Top 1M](https://blog.majestic.com/development/majestic-million-csv-daily/) with recursive depth of scraping of level 1.\n",
    "\n",
    "Malicious\n",
    "- Various blacklists\n",
    "- [openphish](https://openphish.com/)\n",
    "- [phishtank](https://www.phishtank.com/)\n",
    "- [public GitHub faizann24](https://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs)\n",
    "- some more sources\n",
    "\n",
    "The dataset is perfectly balanced (50% benign and 50% malicious). We emphasized on getting benign URLs with paths and not just the domain. Furthermore, depending on your environment you can choose between a smaller subset (```url_data_small.csv``` containing 4000 URLs balanced) or the full data set (```url_data_full.csv``` containing 87380 URLs balanced).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "DATA_HOME = '../data/'\n",
    "df = pd.read_csv(DATA_HOME + 'url_data_full.csv')\n",
    "# df = pd.read_csv(DATA_HOME + 'url_data_small.csv')\n",
    "df.isIP = df.isIP.astype(int)\n",
    "print(df.shape)\n",
    "df.sample(n=5).head() # print a random sample of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isMalicious.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Feature Engineering\n",
    "\n",
    "\n",
    "The traditional approach is to hand-craft Machine Learning features. This can be the most tedious part and often requires extensive domain expertise and data wrangling skills.\n",
    "\n",
    "Previous academic research on identifying malicious or suspicious URLs has focused on studying the usefulness of an exhausted list of candidate features. Here, we cover only a selection of some basic and most widely used features.\n",
    "\n",
    "There are 4 main \"URL Features\" families:\n",
    "1. **BlackList Features**: Check if in any BlackList. BlackLists suffer from a high false negative rate, but can still be useful as a feature.\n",
    "2. **Lexical Features**: Using methods from Natural Language Processing. They capture the property of malicious URLs tending to \"look different\" from benign URLs. Therefore, lexical features quantify contextual information such as the length of the URL.\n",
    "3. **Host-based Features**: They quantify properties of the web site host and answer \"where\" the site is hosted, \"who\" owns it and \"how\" it is managed. API queries are needed for this type of features (WHOIS, DNS records). Some example features can be the date of registration, geolocation, autonomous system (AS) number, connection speed or time-to-live (TTL).\n",
    "4. **Content-based Features**: This is one of the less commonly used feature families as it requires the download of the entire web-page, hence execution of the potential malicious site, which can not only be not safe, but also increases the computational cost of deriving features. Features here can be HTML or JavaScript based.\n",
    "\n",
    "Source: Sahoo et al. 2017: [Malicious URL Detection using Machine Learning: A Survey](https://arxiv.org/pdf/1701.07179.pdf)\n",
    "\n",
    "In this notebook, we focus on a selection of **lexical features** and **host-based features**, starting with the lexical ones in the subsequent code cell. The host-based features instructions will follow in the next markdown cell.\n",
    "\n",
    "### Feature Engineering Sub-Section A - Lexical Features\n",
    "\n",
    "\n",
    "**Selection of lexical features**:\n",
    "\n",
    "1. Length of URL [\"Length\"]\n",
    "2. Length of hostname/domain [\"LengthDomain]\n",
    "3. Count of digits [\"DigitsCount\"]\n",
    "4. Entropy of hostname/domain [\"EntropyDomain\"] - use ```H_entropy``` function provided \n",
    "5. Position (or index) of the first digit [\"FirstDigitIndex\"] - use ```firstDigitIndex``` function provided \n",
    "6. Bag-of-words - more details later\n",
    "\n",
    "We provide a couple of helper functions. Please run the following function cell and then continue reading the next markdown cell with more details on how to derive those features. Have fun!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H_entropy (x):\n",
    "    # Calculate Shannon Entropy\n",
    "    return entropy.shannon_entropy(x)\n",
    "\n",
    "def firstDigitIndex( s ):\n",
    "    for i, c in enumerate(s):\n",
    "        if c.isdigit():\n",
    "            return i + 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks - Sub-Section A - Lexical Features\n",
    "\n",
    "Append features to the pandas 2D DataFrame ```df``` with a new column for each feature. Later, simply drop the columns that are not features. Please focus on ```[\"Length\"]```, ```[\"LengthDomain]```, ```[\"DigitsCount\"]```, ```[\"EntropyDomain\"]``` and ```[\"FirstDigitIndex\"]``` here. [pandas.Series.str](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.html), [pandas.Series.replace](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html) and [pandas.Series,apply](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html) and [tldextract](https://pypi.python.org/pypi/tldextract) can be very helpful to quickly derive those features. Functions you need to apply here are provided in above cell.\n",
    "\n",
    "For the ```Bag-of-words``` see next instructions in next markdown cell...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derive simple lexical features\n",
    "\n",
    "### YOUR CODE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks - Sub-Section A - Lexical Features (continued)\n",
    "\n",
    "There are many different approaches of applying ```bag-of-words``` to URLs. Here we suggest the following approach:\n",
    "\n",
    "1. Extract the different portions of the URL (host names (domains), top-level-domains (tlds) [what is TLD](https://en.wikipedia.org/wiki/Top-level_domain), paths) and create separate pandas Series (or Python lists) using the [tldextract](https://pypi.python.org/pypi/tldextract) library.\n",
    "2. (Code for step 2 is provided) Find the top 20 tlds (e.g. ```com```, ```de```, ```ru``` etc) from the data. Then use [sklearn CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) to create ```bag-of-words``` with a custom vocabulary, here the top 20 tlds as parameter input. Use the ```.fit()``` method to train the CountVectorizer model (save the model in a variable, you will need this model later for real-time transformations of new URLs). After this the ```.transform()``` function is applied to the pandas Series or list of tlds. The resulting matrix is dense, therefore ```.toarray()``` is needed to get a regular numpy matrix. You will notice that this numpy matrix is very sparse, that is, contains a lot of zeros. The ```get_feature_names()``` is useful to get not only the vocabulary, but also to know which column of the matrix corresponds to which ```word```.\n",
    "3. Knowing procedures for step 2, please try to do something similar for the domains. However, choose an ```ngram``` approach via setting the following parameters for the CountVectorizer: ```analyzer='char', ngram_range=(3, 4), max_features=30```.\n",
    "4. (Code for step 4 is provided) Again, we provide you with the solution to applying a different CountVectorizer approach to the path using ```analyzer='word', tokenizer=custom_path_tokenizer, max_features=100``` as parameters.\n",
    "5. Feel free to try different approaches.\n",
    "\n",
    "At each step the numpy matrix is converted to a pandas DataFrame and then concatenated to the previous one and so on. That way you can run one cell multiple times without re-concatenating to the original df which would throw errors. At the end simply replace the original df with the df that contains all bag-of-words features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_path(url):\n",
    "    return re.sub('.'.join([tldextract.extract(url).domain, tldextract.extract(url).suffix]), '', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task1: Create separate Series from df.url for the different portions of the URL\n",
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# domains = \n",
    "# tlds = \n",
    "# paths ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task2: Code provided as example\n",
    "\n",
    "n_tlds = 20\n",
    "top_tlds = list(tlds.value_counts().head(n_tlds).keys())\n",
    "top_tlds = [tld if tld is not '' else 'nan' for tld in top_tlds]  # encode empty/missing tld as 'nan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task2: Code provided as example\n",
    "CountVectorizer_tlds = CountVectorizer(analyzer='word', vocabulary=top_tlds)\n",
    "CountVectorizer_tlds = CountVectorizer_tlds.fit(tlds)\n",
    "matrix_dense_tlds = CountVectorizer_tlds.transform(tlds)\n",
    "\n",
    "print(CountVectorizer_tlds.get_feature_names())\n",
    "print(matrix_dense_tlds.shape)\n",
    "print(sum(matrix_dense_tlds.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task2: Code provided as example (see answers notebook to regenerate the sample output)\n",
    "df_tlds = pd.DataFrame(matrix_dense_tlds.toarray(), columns=CountVectorizer_tlds.get_feature_names())\n",
    "# matrix_dense_tlds.toarray() converts dense matrix to a regular matrix, which will be sparse (a lot of zeros)\n",
    "df1 = pd.concat([df, df_tlds],axis=1)\n",
    "print(len(df1.columns))\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task3: Knowing procedures for step 2, please try to do something similar for the domains. \n",
    "#However, choose an ngram approach via setting the following parameters for the \n",
    "#CountVectorizer: analyzer='char', ngram_range=(3, 4), max_features=30.\n",
    "\n",
    "\n",
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_domains=\n",
    "\n",
    "# df2 = pd.concat([df1, df_domains],axis=1)\n",
    "# print(len(df2.columns))\n",
    "# df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_path_tokenizer(path):  # input is a string for one path from one URL\n",
    "    return list(filter(None, re.compile('[\\?\\=/\\._-]').split(path.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task4: CountVectorizer approach for the path\n",
    "\n",
    "### YOUR CODE ###\n",
    "\n",
    "# modify or come up with a new solution!!!\n",
    "\n",
    "\n",
    "\n",
    "CountVectorizer_paths = CountVectorizer(analyzer='word', tokenizer=custom_path_tokenizer, max_features=100)\n",
    "CountVectorizer_paths = CountVectorizer_paths.fit(paths)\n",
    "\n",
    "matrix_dense_paths = CountVectorizer_paths.transform(paths)\n",
    "\n",
    "df_paths = pd.DataFrame(matrix_dense_paths.toarray(), columns=CountVectorizer_paths.get_feature_names())\n",
    "df3 = pd.concat([df2, df_paths],axis=1)\n",
    "print(len(df3.columns))\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Sub-Section B - Host-based Features\n",
    "\n",
    "\n",
    "Derivation of host-based features often requires the use of APIs or querying information from some authoritative source. It took us 2 days to get all whois data for all of our unique domains (see ```domains_created_db.csv``` file). \n",
    "\n",
    "**Selection of host-based features**:\n",
    "\n",
    "1. Time delta between today's date and creation date ['DurationCreated'] (original whois code included at the end of the notebook)\n",
    "2. Check if it is an IP address ['isIP'] - already provided, no feature engineering needed   \n",
    "3. (Time-to-live ['ttl'] - code to query an authoritative nameserver included at the end of the notebook, but not included in preprocessed data set)\n",
    "\n",
    "\n",
    "### Tasks - Sub-Section B - Host-based Features\n",
    "\n",
    "Append features to the pandas 2D DataFrame ```df``` with a new column for each feature. Later, simply drop the columns that are not features. [pandas.to_datetime](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html) with ```errors='coerce'``` is easy to use to convert the ```WHOIS``` info ```[\"created\"]``` to a datetime data type. Make sure to also fillna with zeros! You can then simply subtract the creation date from today's date to derive the ```[\"DurationCreated\"]``` feature. [pandas.Series.dt.day](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.day.html) can be handy to express the time delta in days. \n",
    "\n",
    "After all features have been added to the pandas 2D DataFrame, please drop all columns that are not features etc, here drop ```['url', 'created', 'domain']```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df3\n",
    "### YOUR CODE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df\n",
    "### YOUR CODE ###\n",
    "#drop ['url', 'created', 'domain']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_final.sample(n=5).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakpoint: Load Features and Labels\n",
    "\n",
    "If you got stuck in Part 1, please simply load the feature matrix we prepared for you, so you can move on to Part 2 and train a Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(DATA_HOME + 'url_features_final_df.csv')\n",
    "print(df_final.isMalicious.value_counts())\n",
    "print(len(df_final.columns))\n",
    "df_final.sample(n=5).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dns.query\n",
    "import dns.resolver\n",
    "from dns.exception import DNSException\n",
    "\n",
    "def query_authoritative_ns (domain, log=lambda msg: None, ttl_only=True):\n",
    "\n",
    "    default = dns.resolver.get_default_resolver()\n",
    "    ns = default.nameservers[0]\n",
    "\n",
    "    n = domain.split('.')\n",
    "\n",
    "    for i in range(len(n), 0, -1):\n",
    "        sub = '.'.join(n[i-1:])\n",
    "\n",
    "        log('Looking up %s on %s' % (sub, ns))\n",
    "        query = dns.message.make_query(sub, dns.rdatatype.NS)\n",
    "        response = dns.query.udp(query, ns)\n",
    "\n",
    "        rcode = response.rcode()\n",
    "        if rcode != dns.rcode.NOERROR:\n",
    "            if rcode == dns.rcode.NXDOMAIN:\n",
    "                raise Exception('%s does not exist.' % (sub))\n",
    "            else:\n",
    "                raise Exception('Error %s' % (dns.rcode.to_text(rcode)))\n",
    "\n",
    "        if len(response.authority) > 0:\n",
    "            rrsets = response.authority\n",
    "        elif len(response.additional) > 0:\n",
    "            rrsets = [response.additional]\n",
    "        else:\n",
    "            rrsets = response.answer\n",
    "\n",
    "        # Handle all RRsets, not just the first one\n",
    "        for rrset in rrsets:\n",
    "            for rr in rrset:\n",
    "                if rr.rdtype == dns.rdatatype.SOA:\n",
    "                    print('Same server is authoritative for %s' % (sub))\n",
    "                elif rr.rdtype == dns.rdatatype.A:\n",
    "                    ns = rr.items[0].address\n",
    "                    print('Glue record for %s: %s' % (rr.name, ns))\n",
    "                elif rr.rdtype == dns.rdatatype.NS:\n",
    "                    authority = rr.target\n",
    "                    ns = default.query(authority).rrset[0].to_text()\n",
    "                    print('%s [%s] is authoritative for %s; ttl %i' % (authority, ns, sub, rrset.ttl))\n",
    "                    result = rrset\n",
    "                    if ttl_only:\n",
    "                        print(rrset)\n",
    "                        result = rrset.ttl\n",
    "                else:\n",
    "                    # IPv6 glue records etc\n",
    "                    #log('Ignoring %s' % (rr))\n",
    "                    pass\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue record for a.gtld-servers.net.: 192.5.6.30\n",
      "Glue record for d.gtld-servers.net.: 192.31.80.30\n",
      "Glue record for g.gtld-servers.net.: 192.42.93.30\n",
      "Glue record for j.gtld-servers.net.: 192.48.79.30\n",
      "Glue record for m.gtld-servers.net.: 192.55.83.30\n",
      "ns-cloud-b1.googledomains.com. [216.239.32.107] is authoritative for gtkcyber.com; ttl 172800\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b1.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b2.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b3.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b4.googledomains.com.\n",
      "ns-cloud-b2.googledomains.com. [216.239.34.107] is authoritative for gtkcyber.com; ttl 172800\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b1.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b2.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b3.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b4.googledomains.com.\n",
      "ns-cloud-b3.googledomains.com. [216.239.36.107] is authoritative for gtkcyber.com; ttl 172800\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b1.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b2.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b3.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b4.googledomains.com.\n",
      "ns-cloud-b4.googledomains.com. [216.239.38.107] is authoritative for gtkcyber.com; ttl 172800\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b1.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b2.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b3.googledomains.com.\n",
      "gtkcyber.com. 172800 IN NS ns-cloud-b4.googledomains.com.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "172800"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_authoritative_ns('www.gtkcyber.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'domain_name': ['GTKCYBER.COM', 'gtkcyber.com'],\n",
       " 'registrar': 'Google LLC',\n",
       " 'whois_server': 'whois.google.com',\n",
       " 'referral_url': None,\n",
       " 'updated_date': datetime.datetime(2018, 1, 20, 9, 7, 56),\n",
       " 'creation_date': datetime.datetime(2017, 1, 20, 3, 32, 35),\n",
       " 'expiration_date': datetime.datetime(2019, 1, 20, 3, 32, 35),\n",
       " 'name_servers': ['NS-CLOUD-B1.GOOGLEDOMAINS.COM',\n",
       "  'NS-CLOUD-B2.GOOGLEDOMAINS.COM',\n",
       "  'NS-CLOUD-B3.GOOGLEDOMAINS.COM',\n",
       "  'NS-CLOUD-B4.GOOGLEDOMAINS.COM'],\n",
       " 'status': ['ok https://icann.org/epp#ok', 'ok https://www.icann.org/epp#ok'],\n",
       " 'emails': ['registrar-abuse@google.com', 'lxhotryq6rne@contactprivacy.email'],\n",
       " 'dnssec': 'unsigned',\n",
       " 'name': 'Contact Privacy Inc. Customer 1241054753',\n",
       " 'org': 'Contact Privacy Inc. Customer 1241054753',\n",
       " 'address': '96 Mowat Ave',\n",
       " 'city': 'Toronto',\n",
       " 'state': 'ON',\n",
       " 'zipcode': 'M4K 3K1',\n",
       " 'country': 'CA'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whois.whois('www.gtkcyber.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whois.whois('www.gtkcyber.com')['creation_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Visualizing the Features\n",
    "In the last step, you're going to explore the feature space to see which features are potentially useful or not and of course whether there is too much noise to make predictions.  \n",
    "\n",
    "First, using [Yellowbrick](http://pythonhosted.org/yellowbrick/examples/examples.html), create a Covariance ranking of the features.  Since this section is about visualizing this information and not deriving it, please execute the cell below so that everyone will have the same data and get the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "DATA_HOME = '../data/'\n",
    "df_final = pd.read_csv(DATA_HOME + 'url_features_final_df.csv')\n",
    "features = df_final.loc[:,'isIP':]\n",
    "target = df_final['isMalicious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the covariance ranking here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What did you see?\n",
    "If you did this correctly, you should see that most of the features are nearly useless. Next, pick 7 features yourself, either using the `feature_selection` functions in `scikit-learn` or by just picking them yourself, and create a pair plot using Seaborn to determine whether there are clear class boundaries between the classes in these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets an arrary of the best features in 1 step.\n",
    "best_features = SelectKBest( score_func=chi2, k=7).fit_transform(features,target)\n",
    "\n",
    "#Get the feature names and indexes\n",
    "best = SelectKBest( score_func=chi2, k=7).fit(features,target)\n",
    "feature_names = pd.Series(features.columns)\n",
    "feature_names[best.get_support()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Seaborn to create a pairplot here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
