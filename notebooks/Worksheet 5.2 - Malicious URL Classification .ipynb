{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Libraries - Make sure to run this cell!\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime  \n",
    "from collections import Counter\n",
    "from sklearn import feature_extraction, tree, model_selection, metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from yellowbrick.classifier import ConfusionMatrix, ROCAUC\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "import scikitplot as skplt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "\n",
    "# Useful cyber libraries\n",
    "import whois   # pip install python-whois\n",
    "import tldextract  # pip install tldextract \n",
    "import ipaddress  # pip install ipaddress\n",
    "import dns.query  # pip install dnspython\n",
    "import dns.resolver\n",
    "import entropy\n",
    "from dns.exception import DNSException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../img/logo_white_bkg_small.png\" align=\"right\" />   \n",
    "\n",
    "\n",
    "## Worksheet 5.2 Malicious URL Classification\n",
    "\n",
    "This worksheet is a step-by-step guide on how to train a Machine Learning model that can detect malicious URLs. We will walk you through the process of transforming raw URL strings to Machine Learning features and creating a Decision Tree Classifer which you will use to determine whether a given URL is malicious or not. Once you have implemented the classifier, the worksheet will walk you through evaluating your model.  \n",
    "\n",
    "### Overview 2 main steps:\n",
    "\n",
    "1. **Feature Engineering** - from raw URL strings to features using [pandas](http://pandas.pydata.org/pandas-docs/stable/) DataFrame, datetime and [numpy](http://www.numpy.org/) manipulations.\n",
    "2. **Machine Learning Classification** - predict whether a URL is malicious or not using a Decision Tree Classifier in [sklearn](http://scikit-learn.org/stable/) and evaluate model performance\n",
    "\n",
    "We provide an additional notebook where you can see how to use \"Featureless Deep Learning\" to build such a classifier.\n",
    "\n",
    "\n",
    "### Data\n",
    "\n",
    "The dataset was build from various different open source data sources. Computationally intensive tasks such as retrieving the creation time for each unique domain in the data set via [whois](https://pypi.python.org/pypi/python-whois) have already been performed beforehand. Some of the open source URLs came with the zone apex only, others didn't include the protocol, therefore, we uniformly removed the protocol (http:// or https://) and subdomain (e.g. www) from the URL string if applicable.\n",
    "\n",
    "Benign\n",
    "- Custom automated webscraping of [Alexa Top 1M](https://blog.majestic.com/development/majestic-million-csv-daily/) with recursive depth of scraping of level 1.\n",
    "\n",
    "Malicious\n",
    "- Various blacklists\n",
    "- [openphish](https://openphish.com/)\n",
    "- [phishtank](https://www.phishtank.com/)\n",
    "- [public GitHub faizann24](https://github.com/faizann24/Using-machine-learning-to-detect-malicious-URLs)\n",
    "- some more sources\n",
    "\n",
    "The dataset is perfectly balanced (50% benign and 50% malicious). We emphasized on getting benign URLs with paths and not just the domain. Furthermore, depending on your environment you can choose between a smaller subset (```url_data_small.csv``` containing 4000 URLs balanced) or the full data set (```url_data_full.csv``` containing 87380 URLs balanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data\n",
    "DATA_HOME = '../data/'\n",
    "df = pd.read_csv(DATA_HOME + 'url_data_full.csv')\n",
    "# df = pd.read_csv('../../Data/URL/url_data_small.csv')\n",
    "df.isIP = df.isIP.astype(int)\n",
    "print(df.shape)\n",
    "df.sample(n=5).head() # print a random sample of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breakpoint: Load Features and Labels\n",
    "\n",
    "If you got stuck in Part 1, please simply load the feature matrix we prepared for you, so you can move on to Part 2 and train a Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.read_csv(DATA_HOME + 'url_features_final_df.csv')\n",
    "print(df_final.isMalicious.value_counts())\n",
    "print(len(df_final.columns))\n",
    "df_final.sample(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 - Machine Learning\n",
    "\n",
    "To learn simple classification procedures using [sklearn](http://scikit-learn.org/stable/) we have split the work flow into 5 steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare Feature matrix ```X``` and ```target``` vector containing the URL labels\n",
    "\n",
    "- X is the feature matrix\n",
    "- target is a vector containing the labels for each URL (often also called *y* in statistics)\n",
    "- for sklearn input X and target can either be a pandas DataFrame/Series or numpy array/vector respectively (can't be lists!)\n",
    "\n",
    "Tasks:\n",
    "- assign ```'isMalicious'``` column to a pandas Series named 'target'\n",
    "- drop ```'isMalicious'``` column from DataFrame and name the resulting pandas DataFrame ```X```\n",
    "- save the column names of the ```X``` in a variable called ```feature_names```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your Code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Simple Cross-Validation\n",
    "\n",
    "Tasks:\n",
    "- split your feature matrix X and target vector into train and test subsets using sklearn [model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html). Sklearn documentation uses also ```X``` to represent the feature matrix of shape [n_samples, n_features], but ```y``` to represent the labels, which we call ```target``` here of shape [n_samples] or [n_samples, n_outputs]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Cross-Validation: Split the data set into training and test data\n",
    "X_train, X_test, target_train, target_test = #Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Train the model and make a prediction\n",
    "\n",
    "Finally, we have prepared and segmented the data. Let's start classifying!!   \n",
    "\n",
    "Tasks:\n",
    "\n",
    "-  Use the sklearn [tree.DecisionTreeClassfier()](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), create a decision tree with standard parameters, and train it using the ```.fit()``` function with ```X_train``` and ```target_train``` data.\n",
    "-  Next, pull a random row from the ```X_test``` array and ```target_test``` vector and see if your classifier got it correct by using the ```.predict()``` function on your classifier with ```test_feature``` as input to this method. When you extract only one random row from the numpy array you'll have to apply ```.reshape(1, -1)``` before passing it into the ```.predict()``` method.\n",
    "\n",
    "If you are interested in trying a real unknown domain, you'll have to create a function to generate the features for that domain before you run it through the classifier (see function ```is_malicious``` a few cells below). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the decision tree based on the entropy criterion\n",
    "\n",
    "\n",
    "# Extract a row from the test data\n",
    "\n",
    "\n",
    "# Make the prediction\n",
    "\n",
    "\n",
    "\n",
    "print('Predicted class:', pred)\n",
    "print('Accurate prediction?', pred[0] == test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional helper functions for is_malicious function\n",
    "def ip_matcher(address):\n",
    "    # Used to validate if string is an ipaddress, currently only IPv4 supported\n",
    "    ip = re.match(\n",
    "        '^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$', address)\n",
    "    if ip:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def strip_http(s):\n",
    "    pattern = re.compile('(http[s]?://)')\n",
    "    clean_url = pattern.sub('', s)\n",
    "    \n",
    "    try:\n",
    "        if clean_url[-1] in ['/']:\n",
    "            clean_url = clean_url[:-1]\n",
    "        if tldextract.extract(clean_url).subdomain:\n",
    "            clean_url = re.sub(tldextract.extract(clean_url).subdomain + '.', '', clean_url)\n",
    "    except:\n",
    "        pass\n",
    "    return clean_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load database of creation dates of various unqiue domains from our data set\n",
    "domains_created_db = pd.read_csv(DATA_HOME + 'domains_created_db.csv')\n",
    "domains_created_db.created = pd.to_datetime(domains_created_db.created, errors='coerce')\n",
    "domains_created_db.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Vectorizer models from part 1\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "# Note all functions for feature calcuations need to be again available here to make new predictions on new URLs\n",
    "\n",
    "def custom_path_tokenizer(path):  # input one path from one URL\n",
    "    return list(filter(None, re.compile('[\\?\\=/\\._-]').split(path.lower()))) \n",
    "\n",
    "def extract_path(url):\n",
    "    return re.sub('.'.join([tldextract.extract(url).domain, tldextract.extract(url).suffix]), '', url)\n",
    "\n",
    "def H_entropy (x):\n",
    "    # Calculate Shannon Entropy\n",
    "    return entropy.shannon_entropy(x)\n",
    "\n",
    "def firstDigitIndex( s ):\n",
    "    for i, c in enumerate(s):\n",
    "        if c.isdigit():\n",
    "            return i + 1\n",
    "    return 0\n",
    "\n",
    "models = [\"feature_names\",\"CountVectorizer_tlds\",\"CountVectorizer_domains\",\"CountVectorizer_paths\"]\n",
    "\n",
    "tmp_models = []\n",
    "for key in models:\n",
    "\n",
    "    with open(DATA_HOME + key + '.pickle', 'rb') as f:\n",
    "            tmp_models.append(pickle.load(f))\n",
    "        \n",
    "feature_names = tmp_models[0]\n",
    "CountVectorizer_tlds = tmp_models[1]\n",
    "CountVectorizer_domains = tmp_models[2]\n",
    "CountVectorizer_paths = tmp_models[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_malicious(url, clf, feature_names, domains_created_db, CountVectorizer_tlds, CountVectorizer_domains, CountVectorizer_paths):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "    - new raw URL string\n",
    "    - trained model/classifiers 'clf'\n",
    "    - feature_names to correctly identify index in numpy array\n",
    "    - domains_created_db: custom database (from .csv file) containing creation date of unique domains of our data set\n",
    "    - trained model CountVectorizer_tlds\n",
    "    - trained model CountVectorizer_domains\n",
    "    - trained model CountVectorizer_paths\n",
    "    \n",
    "    OUTPUT:\n",
    "    - binary prediction (int 0 or 1, where 1 means URL is most likely to be malicious)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # clean raw URL\n",
    "    url = strip_http(url)  # make sure strip_http function is available\n",
    "\n",
    "    # extract portions of URL and retrieve create date from database\n",
    "    domain = tldextract.extract(url).domain\n",
    "    tld = tldextract.extract(url).suffix\n",
    "    path = extract_path(url)  # make sure extract_path function is available\n",
    "    created = domains_created_db.created[domains_created_db.url == '.'.join([domain, tld])]\n",
    "\n",
    "    try:\n",
    "        delta = (pd.to_datetime(datetime.date.today()) - created).dt.days.values[0]\n",
    "    except:\n",
    "        delta = 0\n",
    "\n",
    "    # initialize numpy array\n",
    "    n_features = len(feature_names)\n",
    "    url_features = np.empty(shape=(1, n_features))\n",
    "    \n",
    "    # lexical features simple\n",
    "    url_features[0, feature_names.index('Length')] = len(url)\n",
    "    url_features[0, feature_names.index('LengthDomain')] = len(domain)\n",
    "    pattern = re.compile('([0-9])')\n",
    "    url_features[0, feature_names.index('DigitsCount')] = len(re.findall(pattern, url))\n",
    "    url_features[0, feature_names.index('EntropyDomain')] = H_entropy(domain)  # make sure H_entropy function is available\n",
    "    url_features[0, feature_names.index('FirstDigitIndex')] = firstDigitIndex(url)  # make sure firstDigitIndex function is available\n",
    "   \n",
    "    # lexical features bag-of-words\n",
    "    new_row_tlds = CountVectorizer_tlds.transform([tld]).toarray()\n",
    "    start = feature_names.index(CountVectorizer_tlds.get_feature_names()[0])\n",
    "    stop = feature_names.index(CountVectorizer_tlds.get_feature_names()[-1])                            \n",
    "    url_features[0, start:stop+1] = new_row_tlds\n",
    "    \n",
    "    new_row_domains = CountVectorizer_domains.transform([domain]).toarray()\n",
    "    start = feature_names.index(CountVectorizer_domains.get_feature_names()[0])\n",
    "    stop = feature_names.index(CountVectorizer_domains.get_feature_names()[-1])                            \n",
    "    url_features[0, start:stop+1] = new_row_domains\n",
    "    \n",
    "    new_row_paths = CountVectorizer_paths.transform([path]).toarray()\n",
    "    start = feature_names.index(CountVectorizer_paths.get_feature_names()[0])\n",
    "    stop = feature_names.index(CountVectorizer_paths.get_feature_names()[-1])                            \n",
    "    url_features[0, start:stop+1] = new_row_paths\n",
    "    \n",
    "    # host-based features\n",
    "    url_features[0, feature_names.index('DurationCreated')] = delta\n",
    "    url_features[0, feature_names.index('isIP')] = ip_matcher(url)\n",
    "    \n",
    "    \n",
    "    pred = clf.predict(url_features)\n",
    "    return pred[0]\n",
    "\n",
    "\n",
    "test_URL = 'https://www.google.com'\n",
    "\n",
    "print('Prediction of domain %s\\nis %s [0 means \\\"benign\\\" and 1 \\\"is malicious\\\"]: ' \\\n",
    "%(test_URL, is_malicious(test_URL, clf, feature_names, domains_created_db, CountVectorizer_tlds,\\\n",
    "                         CountVectorizer_domains, CountVectorizer_paths)))  \n",
    "\n",
    "# print('\\nTiming result of new prediction:\\n')\n",
    "# result = %timeit -o is_malicious(test_URL, clf, feature_names, domains_created_db, CountVectorizer_tlds,\\\n",
    "#                          CountVectorizer_domains, CountVectorizer_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Assess model accuracy with simple cross-validation\n",
    "\n",
    "Tasks:\n",
    "- Use sklearn [metrics.accuracy_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to determine your models accuracy. Detailed Instruction:\n",
    "    - Use your trained model to predict the labels of your test data ```X_test```. Run ```.predict()``` method on the clf with your test data ```X_test``` and store the results in a variable called ```target_pred```.. \n",
    "    - Then calculate the accuracy using ```target_test``` (which are the true labels/groundtruth) AND your models predictions on the test portion ```target_pred``` as inputs (in that order ```target_test, target_pred```). The advantage here is to see how your model performs on new data it has not been seen during the training phase.\n",
    "    \n",
    "- Finally print out the confusion matrix using [metrics.confusion_matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair approach: make prediction on test data portion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize this using Yellowbrick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification Report...neat summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Yellowbrick classification report\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use Yellowbrick to visualize the ROC/AUC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Assess model accuracy with k-fold cross-validation\n",
    "\n",
    "Tasks:\n",
    "- Partition the dataset into *k* different subsets\n",
    "- Create *k* different models by training on *k-1* subsets and testing on the remaining subsets\n",
    "- Measure the performance on each of the models and take the average measure.\n",
    "\n",
    "*Short-Cut*\n",
    "All of these steps can be easily achieved by simply using sklearn's [model_selection.KFold()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) and [model_selection.cross_val_score()](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get avergage score +- Standard Error (https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.sem.html)\n",
    "from scipy.stats import sem\n",
    "def mean_score( scores ):\n",
    "    return \"Mean score: {0:.3f} (+/- {1:.3f})\".format( np.mean(scores), sem( scores ))\n",
    "print( mean_score( scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Get the best predictors\n",
    "\n",
    "Tasks:\n",
    "- Using your fitted clf (here DecisionTreeClassifier) get the attribute [feature\\_importances\\_](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)\n",
    "- Create a new df ```f_imp``` with the column names of your features (not including isMalicious column of original df) and transpose ```f_imp```.\n",
    "- Sort [pd.sort\\_vaues](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html) the new df ```f_imp```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_imp = pd.DataFrame([clf.feature_importances_, feature_names]).T.sort_values(0, ascending=False)\n",
    "f_imp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Feature Importances\n",
    "You can also visualize feature importances using scikit-plot.  The code for this is:\n",
    "\n",
    "```python\n",
    "skplt.estimators.plot_feature_importances(clf, feature_names=feature_names)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "From this you really can tell which features are contributing and which are not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Optional) Visualizing your Tree\n",
    "As an optional step, you can actually visualize your tree.  The following code will generate a graph of your decision tree.  You will need graphviz (http://www.graphviz.org) and pydotplus (or pydot) installed for this to work.\n",
    "The Griffon VM has this installed already, but if you try this on a Mac, or Linux machine you will need to install graphviz.\n",
    "\n",
    "**NOTE: This might time out with a large tree.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These libraries are used to visualize the decision tree and require that you have GraphViz\n",
    "# and pydot or pydotplus installed on your computer.\n",
    "\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.core.display import Image\n",
    "import pydotplus as pydot\n",
    "\n",
    "\n",
    "dot_data = StringIO() \n",
    "tree.export_graphviz(clf, out_file=dot_data, \n",
    "                     feature_names=feature_names,\n",
    "                    filled=True, rounded=True,  \n",
    "                    special_characters=True) \n",
    "\n",
    "graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
